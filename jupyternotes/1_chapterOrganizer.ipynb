{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains all the code for creating the HTS and string_dict files in the temp folder for further addition to the MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_words = [\"a\",\"about\",\"above\",\"after\",\"again\",\"against\",\"all\",\"am\",\"an\",\"and\",\"any\",\"are\",\"aren't\",\"as\",\"at\",\"be\",\"because\",\"been\",\"before\",\"being\",\"below\",\"between\",\"both\",\"but\",\"by\",\"can't\",\"cannot\",\"could\",\"couldn't\",\"did\",\"didn't\",\"do\",\"does\",\"doesn't\",\"doing\",\"don't\",\"down\",\"during\",\"each\",\"few\",\"for\",\"from\",\"further\",\"had\",\"hadn't\",\"has\",\"hasn't\",\"have\",\"haven't\",\"having\",\"he\",\"he'd\",\"he'll\",\"he's\",\"her\",\"here\",\"here's\",\"hers\",\"herself\",\"him\",\"himself\",\"his\",\"how\",\"how's\",\"i\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"if\",\"in\",\"into\",\"is\",\"isn't\",\"it\",\"it's\",\"its\",\"itself\",\"let's\",\"me\",\"more\",\"most\",\"mustn't\",\"my\",\"myself\",\"no\",\"nor\",\"not\",\"of\",\"off\",\"on\",\"once\",\"only\",\"or\",\"other\",\"ought\",\"our\",\"ours\",\"ourselves\",\"out\",\"over\",\"own\",\"same\",\"shan't\",\"she\",\"she'd\",\"she'll\",\"she's\",\"should\",\"shouldn't\",\"so\",\"some\",\"such\",\"than\",\"that\",\"that's\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"there\",\"there's\",\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"this\",\"those\",\"through\",\"to\",\"too\",\"under\",\"until\",\"up\",\"very\",\"was\",\"wasn't\",\"we\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"were\",\"weren't\",\"what\",\"what's\",\"when\",\"when's\",\"where\",\"where's\",\"which\",\"while\",\"who\",\"who's\",\"whom\",\"why\",\"why's\",\"with\",\"won't\",\"would\",\"wouldn't\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\"]\n",
    "\n",
    "punctuation_pattern = r'[!\\\"#$%&\\'()*+,-./:;<=>?@\\[\\]\\^_`{|}~â€”]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "\n",
    "def addRowsToDataframe(df: pd.DataFrame, row: list):\n",
    "    \"\"\"Helper method used to easily add new rows to a pandas dataframe\n",
    "\n",
    "    Args:\n",
    "        df: pandas.DataFrame\n",
    "            DataFrame to add the new rows to\n",
    "        row (list): List to be added as a row into the dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    df.loc[len(df)] = row\n",
    "\n",
    "def checkKeyWords(string: str) -> bool:\n",
    "    \"\"\"Checks against the keywords list for any matches in the string input\n",
    "\n",
    "    Args:\n",
    "        string (str): String that will be checked\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the string does not coincide with the list, False if there is a match\n",
    "    \"\"\"\n",
    "\n",
    "    for keyword in key_words:\n",
    "        match = re.search(rf'^{keyword}$', string=string)\n",
    "        if(match):\n",
    "            return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def openJSON(path: str) -> list[dict, any]:\n",
    "    \"\"\"Function that opens a JSON file and returns that file as a list\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to the JSON file\n",
    "\n",
    "    Returns:\n",
    "        list: Returns a list object with the JSON information\n",
    "    \"\"\"\n",
    "\n",
    "    with open(path, 'r') as file:\n",
    "        result = json.loads(file.read())\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Misc methods to display info at time of creation of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countDFLength(iterrows: list) -> int:\n",
    "    \"\"\"Counts the number of rows in the original htsdata.json file df\n",
    "\n",
    "    Args:\n",
    "        iterrows (list): List of rows from iterator for counting\n",
    "\n",
    "    Returns:\n",
    "        int: Number of rows of htsdata.json df\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for row in iterrows:\n",
    "        count += 1\n",
    "    \n",
    "    return count\n",
    "\n",
    "def HTSDictProgressCount(current: int, total: int) -> int:\n",
    "\n",
    "    return f'{int((current/total) * 100)}%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createHTSDict(path: str) -> dict[pd.DataFrame, any]:\n",
    "    \"\"\"Method that creates a dictionary object with all the HTS data from the CBP site in JSON format for all chapters.\n",
    "\n",
    "    Args:\n",
    "        path (str): Path of the original HTS JSON file downloaded from CBP\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary object with all the information in the original HTS file\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_json(path)\n",
    "    columns_df = df.columns.tolist()\n",
    "    final_dict = {}\n",
    "    main_numbers_pattern = re.compile(r'^[\\d]{4}')\n",
    "    start = True\n",
    "    current_main_hts = ['']\n",
    "    previous_main_hts = ['']\n",
    "    total_rows = countDFLength(df.iterrows())\n",
    "    current_row = total_rows\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "\n",
    "        current_main_hts = main_numbers_pattern.findall(row['htsno'])\n",
    "\n",
    "        if start:\n",
    "            previous_main_hts = current_main_hts\n",
    "            final_dict[current_main_hts[0]] = pd.DataFrame(columns=columns_df)\n",
    "            addRowsToDataframe(final_dict[current_main_hts[0]], row)\n",
    "\n",
    "        if len(current_main_hts) == 0:\n",
    "            current_main_hts = previous_main_hts\n",
    "\n",
    "        if (previous_main_hts[0] != current_main_hts[0]) and (len(current_main_hts)):\n",
    "            final_dict[current_main_hts[0]] = pd.DataFrame(columns=columns_df)\n",
    "            addRowsToDataframe(final_dict[current_main_hts[0]], row)\n",
    "            previous_main_hts = current_main_hts  \n",
    "        elif start == False:\n",
    "            addRowsToDataframe(final_dict[current_main_hts[0]], row)\n",
    "        start = False\n",
    "\n",
    "        current_row -= 1\n",
    "        print(f'Progress creating dictionary {HTSDictProgressCount(current_row, total_rows)}', end='\\r')\n",
    "\n",
    "    return final_dict\n",
    "\n",
    "def writeFiles(HTS_dict: dict[pd.DataFrame, any], path_hts: str, path_strings: str):\n",
    "    \"\"\"Writes all header sections of HTS codes into a single JSON file into a path_hts folder. And creates a single JSON file containing all keywords with lists of the HTS file names where they are located\n",
    "\n",
    "    Args:\n",
    "        HTS_dict (dict): HTS dictionary object with all the chapter information from CBP\n",
    "        path_hts (str): Path to store the individual HTS JSON files divided by header code\n",
    "        path_strings (str): Path to store the string dictionary into a JSON file\n",
    "    \"\"\"\n",
    "\n",
    "    string_dict = {}\n",
    "    file_dict = {}\n",
    "    file_path = 'string_dict.json'\n",
    "\n",
    "    for key,df in HTS_dict.items():\n",
    "        \n",
    "        file_dict[key] = f'{path_hts}/{key}.json'\n",
    "\n",
    "        try:\n",
    "            df.to_json(file_dict[key], orient='records')\n",
    "            print(f'File {key}.json - written')\n",
    "        except Exception as e:\n",
    "            print(f'Error writing file {key}.json')\n",
    "            print(e)\n",
    "\n",
    "        for row in df.iterrows():\n",
    "            desc = re.sub(pattern=punctuation_pattern, repl='', string=row[1]['description']).lower()\n",
    "            \n",
    "            array_string = desc.split()\n",
    "            \n",
    "            if(len(array_string) <= 0): continue\n",
    "            \n",
    "            for string in array_string:\n",
    "                if(checkKeyWords(string) == False): continue\n",
    "\n",
    "                if(string in string_dict):\n",
    "                    string_dict[string].append(key)\n",
    "                    string_dict[string] = list(set(string_dict[string]))\n",
    "                    print(f'string_dict key \"{string}\" added chapter \"{key}\"')\n",
    "                    continue\n",
    "            \n",
    "                string_dict[string] = []\n",
    "                string_dict[string].append(key)\n",
    "                string_dict[string] = list(set(string_dict[string]))\n",
    "                print(f'string_dict added string \"{string}\" with chapter \"{key}\"')\n",
    "\n",
    "    with open(f'{path_strings}{file_path}', 'w') as json_file:\n",
    "        json.dump(string_dict, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress creating dictionary 68%\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m writeFiles(\u001b[43mcreateHTSDict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../db_hts/htsdata/htsdata.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../db_hts/temp/NEW_test_files/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../db_hts/temp/NEW_test_string_dict/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[19], line 38\u001b[0m, in \u001b[0;36mcreateHTSDict\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     36\u001b[0m     previous_main_hts \u001b[38;5;241m=\u001b[39m current_main_hts  \n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m start \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m---> 38\u001b[0m     \u001b[43maddRowsToDataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcurrent_main_hts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m start \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     41\u001b[0m current_row \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[17], line 15\u001b[0m, in \u001b[0;36maddRowsToDataframe\u001b[1;34m(df, row)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maddRowsToDataframe\u001b[39m(df: pd\u001b[38;5;241m.\u001b[39mDataFrame, row: \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m      7\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Helper method used to easily add new rows to a pandas dataframe\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03m        row (list): List to be added as a row into the dataframe\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m row\n",
      "File \u001b[1;32mc:\\Users\\RICARDO\\Documents\\github\\HTS_query\\venv\\Lib\\site-packages\\pandas\\core\\indexing.py:911\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[0;32m    910\u001b[0m iloc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39miloc\n\u001b[1;32m--> 911\u001b[0m \u001b[43miloc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_with_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\RICARDO\\Documents\\github\\HTS_query\\venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1932\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   1929\u001b[0m     indexer, missing \u001b[38;5;241m=\u001b[39m convert_missing_indexer(indexer)\n\u001b[0;32m   1931\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m missing:\n\u001b[1;32m-> 1932\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_with_indexer_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1935\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloc\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1936\u001b[0m     \u001b[38;5;66;03m# must come after setting of missing\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\RICARDO\\Documents\\github\\HTS_query\\venv\\Lib\\site-packages\\pandas\\core\\indexing.py:2328\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer_missing\u001b[1;34m(self, indexer, value)\u001b[0m\n\u001b[0;32m   2326\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_mgr \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39m_mgr\n\u001b[0;32m   2327\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2328\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_append\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m_mgr\n\u001b[0;32m   2329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_maybe_update_cacher(clear\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\RICARDO\\Documents\\github\\HTS_query\\venv\\Lib\\site-packages\\pandas\\core\\frame.py:10559\u001b[0m, in \u001b[0;36mDataFrame._append\u001b[1;34m(self, other, ignore_index, verify_integrity, sort)\u001b[0m\n\u001b[0;32m  10556\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m  10557\u001b[0m     to_concat \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m, other]\n\u001b[1;32m> 10559\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m  10560\u001b[0m \u001b[43m    \u001b[49m\u001b[43mto_concat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10561\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10562\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10563\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10564\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m  10565\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\RICARDO\\Documents\\github\\HTS_query\\venv\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:395\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    382\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[0;32m    383\u001b[0m     objs,\n\u001b[0;32m    384\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    392\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m    393\u001b[0m )\n\u001b[1;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\RICARDO\\Documents\\github\\HTS_query\\venv\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:684\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    680\u001b[0m             indexers[ax] \u001b[38;5;241m=\u001b[39m obj_labels\u001b[38;5;241m.\u001b[39mget_indexer(new_labels)\n\u001b[0;32m    682\u001b[0m     mgrs_indexers\u001b[38;5;241m.\u001b[39mappend((obj\u001b[38;5;241m.\u001b[39m_mgr, indexers))\n\u001b[1;32m--> 684\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[43mconcatenate_managers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmgrs_indexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_axes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbm_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    688\u001b[0m     new_data\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n",
      "File \u001b[1;32mc:\\Users\\RICARDO\\Documents\\github\\HTS_query\\venv\\Lib\\site-packages\\pandas\\core\\internals\\concat.py:157\u001b[0m, in \u001b[0;36mconcatenate_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m    154\u001b[0m     out\u001b[38;5;241m.\u001b[39maxes \u001b[38;5;241m=\u001b[39m axes\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m--> 157\u001b[0m concat_plan \u001b[38;5;241m=\u001b[39m \u001b[43m_get_combined_plan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmgrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    159\u001b[0m blocks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    160\u001b[0m values: ArrayLike\n",
      "File \u001b[1;32mc:\\Users\\RICARDO\\Documents\\github\\HTS_query\\venv\\Lib\\site-packages\\pandas\\core\\internals\\concat.py:304\u001b[0m, in \u001b[0;36m_get_combined_plan\u001b[1;34m(mgrs)\u001b[0m\n\u001b[0;32m    301\u001b[0m max_len \u001b[38;5;241m=\u001b[39m mgrs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    303\u001b[0m blknos_list \u001b[38;5;241m=\u001b[39m [mgr\u001b[38;5;241m.\u001b[39mblknos \u001b[38;5;28;01mfor\u001b[39;00m mgr \u001b[38;5;129;01min\u001b[39;00m mgrs]\n\u001b[1;32m--> 304\u001b[0m pairs \u001b[38;5;241m=\u001b[39m \u001b[43mlibinternals\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_concat_blkno_indexers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblknos_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ind, (blknos, bp) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pairs):\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;66;03m# assert bp.is_slice_like\u001b[39;00m\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;66;03m# assert len(bp) > 0\u001b[39;00m\n\u001b[0;32m    309\u001b[0m     units_for_bp \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "writeFiles(createHTSDict('../db_hts/htsdata/htsdata.json'), '../db_hts/temp/NEW_test_files/', '../db_hts/temp/NEW_test_string_dict/')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
